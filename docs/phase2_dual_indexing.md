# éšæ®µäºŒ:é›™ç´¢å¼•æ§‹å»º (Dual Indexing)

## 1. ç³»çµ±å®šä½èˆ‡æ ¸å¿ƒåƒ¹å€¼ (Context & Value)

### 1.1 éšæ®µå®šä½

é›™ç´¢å¼•æ§‹å»ºæ˜¯ RAG ç³»çµ±çš„æª¢ç´¢åŸºç¤,è² è²¬ç‚ºéšæ®µä¸€ç”¢å‡ºçš„æ–‡æœ¬å–®å…ƒå»ºç«‹å…©å¥—äº’è£œçš„ç´¢å¼•æ©Ÿåˆ¶:**Dense Index (å‘é‡ç´¢å¼•)** èˆ‡ **Sparse Index (é—œéµå­—ç´¢å¼•)**ã€‚é€™ç¨®æ··åˆæ¶æ§‹èƒ½å¤ åŒæ™‚æ•æ‰èªæ„ç›¸ä¼¼æ€§èˆ‡ç²¾ç¢ºé—œéµå­—åŒ¹é…,æ˜¯å¯¦ç¾é«˜å“è³ªæª¢ç´¢çš„é—œéµã€‚

### 1.2 æ ¸å¿ƒç—›é»

å–®ä¸€ç´¢å¼•æ©Ÿåˆ¶å­˜åœ¨æ˜é¡¯çš„å±€é™æ€§:

- **ç´”å‘é‡æª¢ç´¢ (Dense Only)**:
  - âœ… å„ªå‹¢:èƒ½ç†è§£èªæ„ç›¸ä¼¼æ€§ (å¦‚ "revenue" èˆ‡ "sales" çš„é—œè¯)
  - âŒ åŠ£å‹¢:å°å°ˆæœ‰åè©èˆ‡æ•¸å€¼ä¸æ•æ„Ÿ (å¦‚å…¬å¸ä»£ç¢¼ "AAPL" å¯èƒ½è¢«èª¤åŒ¹é…)
- **ç´”é—œéµå­—æª¢ç´¢ (Sparse Only)**:
  - âœ… å„ªå‹¢:ç²¾ç¢ºåŒ¹é…è¡“èªèˆ‡æ•¸å€¼
  - âŒ åŠ£å‹¢:ç„¡æ³•è™•ç†åŒç¾©è©èˆ‡èªæ„è®ŠåŒ– (å¦‚ "profit" èˆ‡ "net income")

è²¡å‹™å•ç­”å ´æ™¯åŒæ™‚éœ€è¦**èªæ„ç†è§£**èˆ‡**ç²¾ç¢ºåŒ¹é…**,å› æ­¤å¿…é ˆæ¡ç”¨é›™ç´¢å¼•æ¶æ§‹ã€‚

### 1.3 é æœŸæ•ˆæœ

å®Œæˆæœ¬éšæ®µå¾Œ,ç³»çµ±å°‡å…·å‚™:

1. **èªæ„æª¢ç´¢èƒ½åŠ›**:é€é BGE-M3 Embeddings ç†è§£å•é¡Œæ„åœ–
2. **ç²¾ç¢ºåŒ¹é…èƒ½åŠ›**:é€é BM25 ç´¢å¼•å¿«é€Ÿå®šä½é—œéµè©
3. **é«˜æ•ˆæŸ¥è©¢**:å…©å¥—ç´¢å¼•å‡æ”¯æ´æ¯«ç§’ç´šæª¢ç´¢
4. **å¯æ“´å±•æ€§**:ChromaDB æ”¯æ´å¢é‡æ›´æ–°èˆ‡åˆ†æ•£å¼éƒ¨ç½²

---

> **æ¶æ§‹éŠœæ¥èªªæ˜**:
> äº†è§£é›™ç´¢å¼•çš„å¿…è¦æ€§å¾Œ,ä¸‹ä¸€å±¤å°‡èªªæ˜å¦‚ä½•é€éã€ŒEmbedding è¨ˆç®—â†’å‘é‡å„²å­˜â†’BM25 å»ºç«‹ã€çš„æµç¨‹,åœ¨ Kaggle GPU èˆ‡ Local CPU é–“åˆ†å·¥å”ä½œã€‚

---

## 2. å·¥ä½œæµç¨‹èˆ‡æ¶æ§‹ (Workflow & Architecture)

### 2.1 æ•´é«”æµç¨‹

é›™ç´¢å¼•æ§‹å»ºåˆ†ç‚ºå…©æ¢ä¸¦è¡Œè·¯å¾‘:

```
éšæ®µä¸€ç”¢å‡º: chunks.jsonl
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dense Index Pipeline          â”‚   Sparse Index Pipeline         â”‚
â”‚   (Kaggle GPU)                  â”‚   (Local CPU)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. è¼‰å…¥ BGE-M3 æ¨¡å‹             â”‚ 1. è¼‰å…¥ chunks.jsonl            â”‚
â”‚ 2. æ‰¹æ¬¡è¨ˆç®— Embeddings          â”‚ 2. Tokenization (è²¡ç¶“è¡“èªå„ªåŒ–)  â”‚
â”‚ 3. å„²å­˜ç‚º embeddings.npy        â”‚ 3. å»ºç«‹ BM25 å€’æ’ç´¢å¼•           â”‚
â”‚ 4. ä¸Šå‚³è‡³ Kaggle Output         â”‚ 4. å„²å­˜ç‚º bm25_index.pkl        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                                   â†“
ä¸‹è¼‰ embeddings.npy                 è¼‰å…¥ bm25_index.pkl
    â†“                                   â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            æ•´åˆè‡³ ChromaDB
            (Collection: financial_10k)
```

### 2.2 åŸºç¤è¨­æ–½åˆ†å·¥

#### â˜ï¸ Kaggle Cloud Layer (Dense Index)

**ç‚ºä½•åœ¨ Kaggle åŸ·è¡Œ?**

- BGE-M3 æ¨¡å‹éœ€è¦ GPU åŠ é€Ÿ (P100/T4)
- å¤§è¦æ¨¡ Embedding è¨ˆç®—è€—æ™‚ (8000+ Chunks éœ€ 30-60 åˆ†é˜)
- Kaggle æä¾›å…è²» GPU é…é¡ (æ¯é€± 30 å°æ™‚)

**åŸ·è¡Œç’°å¢ƒ**:

- **Notebook**: `embedding_computation.ipynb`
- **GPU**: P100 (16GB VRAM) æˆ– T4 (16GB VRAM)
- **ä¾è³´**: `transformers`, `torch`, `numpy`

#### ğŸ’» Local Desktop Layer (Sparse Index + Integration)

**ç‚ºä½•åœ¨æœ¬åœ°åŸ·è¡Œ?**

- BM25 ç´¢å¼•å»ºç«‹åƒ…éœ€ CPU (ç„¡ GPU éœ€æ±‚)
- ChromaDB æ•´åˆéœ€è¦æœ¬åœ°è³‡æ–™åº«ç’°å¢ƒ
- æ–¹ä¾¿å¾ŒçºŒçš„å¢é‡æ›´æ–°èˆ‡æŸ¥è©¢æ¸¬è©¦

**åŸ·è¡Œç’°å¢ƒ**:

- **Script**: `scripts/build_bm25_index.py`, `scripts/integrate_embeddings.py`
- **ä¾è³´**: `rank-bm25`, `chromadb`, `numpy`

### 2.3 è³‡æ–™æµå‘

```
[Kaggle] chunks.jsonl â†’ BGE-M3 â†’ embeddings.npy (1024-dim vectors)
                                        â†“
                                  Download to Local
                                        â†“
[Local] ChromaDB.add(embeddings=embeddings.npy, documents=chunks)

[Local] chunks.jsonl â†’ Tokenizer â†’ BM25 Index â†’ bm25_index.pkl
                                        â†“
                                  Load for Query
```

---

> **ç´°ç¯€éŠœæ¥èªªæ˜**:
> ç¢ºç«‹äº†ã€ŒKaggle ç”¢è£½ Embeddingsã€Local å»ºç«‹ BM25ã€çš„åˆ†å·¥å¾Œ,ä»¥ä¸‹å°‡æ·±å…¥èªªæ˜ BGE-M3 æ¨¡å‹é…ç½®ã€BM25 åƒæ•¸èª¿å„ªèˆ‡ ChromaDB æ¶æ§‹è¨­è¨ˆã€‚

---

## 3. æŠ€è¡“è¦æ ¼èˆ‡å¯¦ä½œç´°ç¯€ (Detailed Specification)

### 3.1 Dense Index: BGE-M3 Embeddings

#### 3.1.1 æ¨¡å‹é¸æ“‡ç†ç”±

**[BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)** æ˜¯ç›®å‰æœ€é©åˆè²¡å ±å ´æ™¯çš„ Embedding æ¨¡å‹:

| ç‰¹æ€§             | BGE-M3              | ç«¶å“ (OpenAI text-embedding-3) |
| ---------------- | ------------------- | ------------------------------ |
| **æœ€å¤§è¼¸å…¥é•·åº¦** | 8192 tokens         | 8191 tokens                    |
| **è¼¸å‡ºç¶­åº¦**     | 1024-dim            | 1536-dim                       |
| **å¤šèªè¨€æ”¯æ´**   | âœ… (100+ èªè¨€)      | âœ…                             |
| **é ˜åŸŸé©æ‡‰æ€§**   | é€šç”¨ + é‡‘èå¾®èª¿     | é€šç”¨                           |
| **æˆæœ¬**         | å…è²» (è‡ªè¨—ç®¡)       | $0.13/1M tokens                |
| **æ¨ç†é€Ÿåº¦**     | ~50 chunks/sec (T4) | API å»¶é² ~200ms                |

**é—œéµå„ªå‹¢**:

- **é•·æ–‡æœ¬æ”¯æ´**:è²¡å ±æ®µè½å¸¸è¶…é 512 tokens,BGE-M3 å¯å®Œæ•´ç·¨ç¢¼
- **æˆæœ¬æ•ˆç›Š**:åœ¨ Kaggle å…è²» GPU ä¸Šé‹è¡Œ,ç„¡ API è²»ç”¨
- **å¯æ§æ€§**:å¯é‡å°è²¡ç¶“é ˜åŸŸé€²è¡Œ Fine-tuning

#### 3.1.2 Embedding è¨ˆç®—æµç¨‹

**Kaggle Notebook å¯¦ä½œ** (`embedding_computation.ipynb`):

```python
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import json
from tqdm import tqdm

# 1. è¼‰å…¥æ¨¡å‹
model_name = "BAAI/bge-m3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ç§»è‡³ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# 2. è¼‰å…¥ Chunks
chunks = []
with open('/kaggle/input/financial-chunks/chunks.jsonl', 'r') as f:
    for line in f:
        chunks.append(json.loads(line))

# 3. æ‰¹æ¬¡è¨ˆç®— Embeddings
batch_size = 32
embeddings = []

for i in tqdm(range(0, len(chunks), batch_size)):
    batch_texts = [chunk['text'] for chunk in chunks[i:i+batch_size]]

    # Tokenization
    inputs = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=8192,
        return_tensors='pt'
    ).to(device)

    # å‰å‘å‚³æ’­
    with torch.no_grad():
        outputs = model(**inputs)
        # ä½¿ç”¨ [CLS] token çš„ embedding
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    embeddings.append(batch_embeddings)

# 4. åˆä½µä¸¦å„²å­˜
embeddings = np.vstack(embeddings)
np.save('/kaggle/working/embeddings.npy', embeddings)

print(f"âœ… Embeddings shape: {embeddings.shape}")  # (8420, 1024)
```

#### 3.1.3 æ•ˆèƒ½å„ªåŒ–ç­–ç•¥

**1. æ··åˆç²¾åº¦è¨ˆç®— (Mixed Precision)**

```python
from torch.cuda.amp import autocast

with autocast():
    outputs = model(**inputs)
```

**æ•ˆæœ**: æ¸›å°‘ VRAM ä½¿ç”¨ 40%,åŠ é€Ÿæ¨ç† 30%

**2. å‹•æ…‹æ‰¹æ¬¡å¤§å°**

```python
def get_optimal_batch_size(available_vram_gb: float) -> int:
    """æ ¹æ“šå¯ç”¨ VRAM å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°"""
    if available_vram_gb >= 16:
        return 64
    elif available_vram_gb >= 8:
        return 32
    else:
        return 16
```

**3. Checkpoint æ©Ÿåˆ¶**

```python
import os

checkpoint_path = '/kaggle/working/checkpoint.npy'

# æ¯è™•ç† 1000 å€‹ Chunks å„²å­˜ä¸€æ¬¡
if i % 1000 == 0 and i > 0:
    np.save(checkpoint_path, np.vstack(embeddings))
    print(f"ğŸ’¾ Checkpoint saved at {i} chunks")

# å¾ Checkpoint æ¢å¾©
if os.path.exists(checkpoint_path):
    embeddings = [np.load(checkpoint_path)]
    start_idx = len(embeddings[0])
    print(f"ğŸ”„ Resuming from chunk {start_idx}")
```

### 3.2 Sparse Index: BM25

#### 3.2.1 BM25 æ¼”ç®—æ³•åŸç†

**BM25 (Best Matching 25)** æ˜¯ä¸€ç¨®åŸºæ–¼æ©Ÿç‡çš„æ’åºå‡½æ•¸:

$$
\text{Score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
$$

**åƒæ•¸èªªæ˜**:

- $q_i$: æŸ¥è©¢ä¸­çš„ç¬¬ $i$ å€‹è©
- $f(q_i, D)$: è© $q_i$ åœ¨æ–‡ä»¶ $D$ ä¸­çš„è©é »
- $|D|$: æ–‡ä»¶ $D$ çš„é•·åº¦
- $\text{avgdl}$: æ‰€æœ‰æ–‡ä»¶çš„å¹³å‡é•·åº¦
- $k_1$: è©é »é£½å’Œåƒæ•¸ (é è¨­ 1.5)
- $b$: é•·åº¦æ­£è¦åŒ–åƒæ•¸ (é è¨­ 0.75)

#### 3.2.2 è²¡ç¶“é ˜åŸŸ Tokenizer å„ªåŒ–

**æŒ‘æˆ°**: æ¨™æº– Tokenizer æœƒå°‡ "EBITDA" åˆ‡åˆ†ç‚º ["E", "BIT", "DA"],ç ´å£èªæ„ã€‚

**è§£æ±ºæ–¹æ¡ˆ**: è‡ªè¨‚ Tokenizer,ä¿ç•™è²¡ç¶“å°ˆæœ‰åè©ã€‚

```python
import re
from typing import List

class FinancialTokenizer:
    """è²¡ç¶“é ˜åŸŸå°ˆç”¨ Tokenizer"""

    # è²¡ç¶“è¡“èªç™½åå–® (ä¸åˆ‡åˆ†)
    FINANCIAL_TERMS = {
        'EBITDA', 'GAAP', 'EPS', 'P/E', 'ROE', 'ROA', 'CAGR',
        'CAPEX', 'OPEX', 'FCF', 'NPV', 'IRR', 'WACC', 'SEC'
    }

    # å…¬å¸ä»£ç¢¼æ¨¡å¼ (1-5 å€‹å¤§å¯«å­—æ¯)
    TICKER_PATTERN = re.compile(r'\b[A-Z]{1,5}\b')

    # æ•¸å€¼æ¨¡å¼ (ä¿ç•™å®Œæ•´æ•¸å­—,åŒ…å«é€—è™Ÿèˆ‡å°æ•¸é»)
    NUMBER_PATTERN = re.compile(r'\$?\d{1,3}(,\d{3})*(\.\d+)?[BMK]?')

    def tokenize(self, text: str) -> List[str]:
        """
        åˆ‡åˆ†æ–‡æœ¬ç‚º tokens,ä¿ç•™è²¡ç¶“è¡“èªå®Œæ•´æ€§

        Args:
            text: å¾…åˆ‡åˆ†æ–‡æœ¬

        Returns:
            Token åˆ—è¡¨
        """
        tokens = []

        # 1. æå–ä¸¦ä¿ç•™è²¡ç¶“è¡“èª
        for term in self.FINANCIAL_TERMS:
            if term in text:
                text = text.replace(term, f' __{term}__ ')

        # 2. æå–ä¸¦ä¿ç•™å…¬å¸ä»£ç¢¼
        text = self.TICKER_PATTERN.sub(r' __\g<0>__ ', text)

        # 3. æå–ä¸¦ä¿ç•™æ•¸å€¼
        text = self.NUMBER_PATTERN.sub(r' __\g<0>__ ', text)

        # 4. æ¨™æº–åˆ‡åˆ†
        raw_tokens = text.lower().split()

        # 5. é‚„åŸä¿ç•™çš„è¡“èª
        for token in raw_tokens:
            if token.startswith('__') and token.endswith('__'):
                tokens.append(token[2:-2])  # ç§»é™¤æ¨™è¨˜ç¬¦è™Ÿ
            elif len(token) > 2:  # éæ¿¾éçŸ­çš„è©
                tokens.append(token)

        return tokens

# ä½¿ç”¨ç¯„ä¾‹
tokenizer = FinancialTokenizer()
text = "Apple (AAPL) reported EBITDA of $120.5B in Q4 2023"
tokens = tokenizer.tokenize(text)
print(tokens)
# ['apple', 'AAPL', 'reported', 'EBITDA', 'of', '$120.5B', 'in', 'q4', '2023']
```

#### 3.2.3 BM25 ç´¢å¼•å»ºç«‹

**Local Script å¯¦ä½œ** (`scripts/build_bm25_index.py`):

```python
from rank_bm25 import BM25Okapi
import json
import pickle
from pathlib import Path

def build_bm25_index(chunks_path: Path, output_path: Path):
    """
    å»ºç«‹ BM25 ç´¢å¼•

    Args:
        chunks_path: chunks.jsonl è·¯å¾‘
        output_path: ç´¢å¼•å„²å­˜è·¯å¾‘
    """
    # 1. è¼‰å…¥ Chunks
    chunks = []
    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))

    # 2. Tokenization
    tokenizer = FinancialTokenizer()
    tokenized_corpus = [tokenizer.tokenize(chunk['text']) for chunk in chunks]

    # 3. å»ºç«‹ BM25 ç´¢å¼•
    bm25 = BM25Okapi(tokenized_corpus)

    # 4. å„²å­˜ç´¢å¼•
    index_data = {
        'bm25': bm25,
        'chunk_ids': [chunk['chunk_id'] for chunk in chunks],
        'tokenizer': tokenizer
    }

    with open(output_path, 'wb') as f:
        pickle.dump(index_data, f)

    print(f"âœ… BM25 index built: {len(chunks)} chunks")

if __name__ == '__main__':
    build_bm25_index(
        chunks_path=Path('data/processed/chunks.jsonl'),
        output_path=Path('data/indexes/bm25_index.pkl')
    )
```

#### 3.2.4 BM25 åƒæ•¸èª¿å„ª

**åƒæ•¸å½±éŸ¿åˆ†æ**:

| åƒæ•¸   | é è¨­å€¼ | èª¿å„ªæ–¹å‘     | å½±éŸ¿                                |
| ------ | ------ | ------------ | ----------------------------------- |
| **k1** | 1.5    | â†‘ æå‡è‡³ 2.0 | å¢åŠ é«˜é »è©çš„æ¬Šé‡ (é©åˆè¡“èªå¯†é›†æ–‡æœ¬) |
| **b**  | 0.75   | â†“ é™è‡³ 0.5   | æ¸›å°‘é•·åº¦æ‡²ç½° (é©åˆé•·æ–‡æœ¬)           |

**èª¿å„ªå¯¦é©—**:

```python
from sklearn.model_selection import ParameterGrid

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'k1': [1.2, 1.5, 1.8, 2.0],
    'b': [0.5, 0.6, 0.75, 0.85]
}

# è©•ä¼°å‡½æ•¸ (ä½¿ç”¨é©—è­‰é›†)
def evaluate_bm25(k1: float, b: float, val_queries: List[str], val_relevance: List[List[int]]):
    """è©•ä¼° BM25 åƒæ•¸çµ„åˆçš„ MRR (Mean Reciprocal Rank)"""
    bm25 = BM25Okapi(tokenized_corpus, k1=k1, b=b)

    reciprocal_ranks = []
    for query, relevant_ids in zip(val_queries, val_relevance):
        scores = bm25.get_scores(tokenizer.tokenize(query))
        ranked_ids = np.argsort(scores)[::-1]

        # æ‰¾åˆ°ç¬¬ä¸€å€‹ç›¸é—œæ–‡ä»¶çš„æ’å
        for rank, doc_id in enumerate(ranked_ids, 1):
            if doc_id in relevant_ids:
                reciprocal_ranks.append(1 / rank)
                break

    return np.mean(reciprocal_ranks)

# ç¶²æ ¼æœå°‹
best_score = 0
best_params = {}

for params in ParameterGrid(param_grid):
    score = evaluate_bm25(**params, val_queries=queries, val_relevance=relevance)
    if score > best_score:
        best_score = score
        best_params = params

print(f"Best params: {best_params}, MRR: {best_score:.4f}")
```

### 3.3 ChromaDB æ•´åˆ

#### 3.3.1 Collection æ¶æ§‹è¨­è¨ˆ

**Collection å‘½å**: `financial_10k_v1`

**Schema**:

```python
import chromadb
from chromadb.config import Settings

# åˆå§‹åŒ– ChromaDB
client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="data/chromadb"
))

# å»ºç«‹ Collection
collection = client.create_collection(
    name="financial_10k_v1",
    metadata={
        "description": "10-K Financial Reports RAG System",
        "embedding_model": "BAAI/bge-m3",
        "embedding_dim": 1024,
        "total_chunks": 8420,
        "created_at": "2024-01-30"
    }
)
```

#### 3.3.2 è³‡æ–™åŒ¯å…¥æµç¨‹

```python
import numpy as np
import json

def integrate_embeddings_to_chromadb(
    chunks_path: Path,
    embeddings_path: Path,
    collection_name: str
):
    """
    å°‡ Embeddings æ•´åˆè‡³ ChromaDB

    Args:
        chunks_path: chunks.jsonl è·¯å¾‘
        embeddings_path: embeddings.npy è·¯å¾‘
        collection_name: ChromaDB Collection åç¨±
    """
    # 1. è¼‰å…¥è³‡æ–™
    chunks = []
    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))

    embeddings = np.load(embeddings_path)

    # 2. æº–å‚™ ChromaDB æ ¼å¼
    ids = [chunk['chunk_id'] for chunk in chunks]
    documents = [chunk['text'] for chunk in chunks]
    metadatas = [chunk['metadata'] for chunk in chunks]
    embeddings_list = embeddings.tolist()

    # 3. æ‰¹æ¬¡åŒ¯å…¥ (é¿å…è¨˜æ†¶é«”æº¢å‡º)
    batch_size = 1000
    collection = client.get_collection(collection_name)

    for i in range(0, len(ids), batch_size):
        collection.add(
            ids=ids[i:i+batch_size],
            documents=documents[i:i+batch_size],
            metadatas=metadatas[i:i+batch_size],
            embeddings=embeddings_list[i:i+batch_size]
        )
        print(f"âœ… Imported batch {i//batch_size + 1}/{len(ids)//batch_size + 1}")

    print(f"âœ… Total chunks in ChromaDB: {collection.count()}")
```

#### 3.3.3 ç´¢å¼•æ•ˆèƒ½å„ªåŒ–

**1. HNSW ç´¢å¼•åƒæ•¸**

ChromaDB é è¨­ä½¿ç”¨ HNSW (Hierarchical Navigable Small World) æ¼”ç®—æ³•:

```python
collection = client.create_collection(
    name="financial_10k_v1",
    metadata={
        "hnsw:space": "cosine",           # è·é›¢åº¦é‡ (cosine/l2/ip)
        "hnsw:construction_ef": 200,      # å»ºç«‹ç´¢å¼•æ™‚çš„æœå°‹ç¯„åœ (â†‘ æå‡å“è³ªä½†è®Šæ…¢)
        "hnsw:M": 16                      # æ¯å€‹ç¯€é»çš„é€£æ¥æ•¸ (â†‘ æå‡å¬å›ç‡ä½†å¢åŠ è¨˜æ†¶é«”)
    }
)
```

**åƒæ•¸èª¿å„ªå»ºè­°**:

| åƒæ•¸              | é è¨­å€¼ | è²¡å ±å ´æ™¯å»ºè­° | ç†ç”±                           |
| ----------------- | ------ | ------------ | ------------------------------ |
| `construction_ef` | 100    | 200          | è²¡å ±æŸ¥è©¢è¦æ±‚é«˜å¬å›ç‡           |
| `M`               | 16     | 32           | å¢åŠ é€£æ¥æ•¸ä»¥æå‡é•·æ–‡æœ¬æª¢ç´¢å“è³ª |
| `space`           | `l2`   | `cosine`     | BGE-M3 å»ºè­°ä½¿ç”¨ Cosine è·é›¢    |

**2. æŸ¥è©¢æ™‚åƒæ•¸**

```python
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=20,
    include=["documents", "metadatas", "distances"]
)
```

### 3.4 éŒ¯èª¤è™•ç†èˆ‡é©—è­‰

#### 3.4.1 Embedding å“è³ªæª¢æŸ¥

```python
def validate_embeddings(embeddings: np.ndarray):
    """é©—è­‰ Embeddings å“è³ª"""

    # 1. æª¢æŸ¥å½¢ç‹€
    assert embeddings.shape[1] == 1024, f"Expected 1024-dim, got {embeddings.shape[1]}"

    # 2. æª¢æŸ¥ NaN/Inf
    assert not np.isnan(embeddings).any(), "Embeddings contain NaN"
    assert not np.isinf(embeddings).any(), "Embeddings contain Inf"

    # 3. æª¢æŸ¥å‘é‡ç¯„æ•¸ (æ‡‰æ¥è¿‘ 1,å› ç‚º BGE-M3 è¼¸å‡ºå·²æ­£è¦åŒ–)
    norms = np.linalg.norm(embeddings, axis=1)
    assert np.allclose(norms, 1.0, atol=0.1), f"Abnormal norms: {norms.min():.3f} - {norms.max():.3f}"

    # 4. æª¢æŸ¥å¤šæ¨£æ€§ (é¿å…æ‰€æœ‰å‘é‡éæ–¼ç›¸ä¼¼)
    similarity_matrix = np.dot(embeddings, embeddings.T)
    avg_similarity = (similarity_matrix.sum() - len(embeddings)) / (len(embeddings) * (len(embeddings) - 1))
    assert avg_similarity < 0.8, f"Embeddings too similar: {avg_similarity:.3f}"

    print("âœ… Embeddings validation passed")
```

#### 3.4.2 ç´¢å¼•ä¸€è‡´æ€§æª¢æŸ¥

```python
def verify_index_consistency(
    chunks_path: Path,
    chromadb_collection,
    bm25_index_path: Path
):
    """é©—è­‰ä¸‰ä»½è³‡æ–™çš„ä¸€è‡´æ€§"""

    # 1. è¼‰å…¥è³‡æ–™
    with open(chunks_path, 'r') as f:
        chunks = [json.loads(line) for line in f]

    with open(bm25_index_path, 'rb') as f:
        bm25_data = pickle.load(f)

    chromadb_count = chromadb_collection.count()

    # 2. æª¢æŸ¥æ•¸é‡ä¸€è‡´æ€§
    assert len(chunks) == chromadb_count, \
        f"Chunks ({len(chunks)}) != ChromaDB ({chromadb_count})"

    assert len(chunks) == len(bm25_data['chunk_ids']), \
        f"Chunks ({len(chunks)}) != BM25 ({len(bm25_data['chunk_ids'])})"

    # 3. æª¢æŸ¥ ID ä¸€è‡´æ€§
    chunk_ids = {chunk['chunk_id'] for chunk in chunks}
    bm25_ids = set(bm25_data['chunk_ids'])

    assert chunk_ids == bm25_ids, \
        f"ID mismatch: {len(chunk_ids - bm25_ids)} missing in BM25"

    print("âœ… Index consistency verified")
```

### 3.5 è¼¸å‡ºèˆ‡äº¤ä»˜ç‰©

#### 3.5.1 Kaggle Output

**æª”æ¡ˆçµæ§‹**:

```
/kaggle/working/
â”œâ”€â”€ embeddings.npy          # (8420, 1024) float32 é™£åˆ—
â”œâ”€â”€ embedding_log.json      # è™•ç†æ—¥èªŒ
â””â”€â”€ checkpoint.npy          # ä¸­é–“æª¢æŸ¥é» (å¯é¸)
```

**embedding_log.json ç¯„ä¾‹**:

```json
{
  "model": "BAAI/bge-m3",
  "total_chunks": 8420,
  "embedding_dim": 1024,
  "batch_size": 32,
  "processing_time_seconds": 1847,
  "gpu_type": "Tesla P100-PCIE-16GB",
  "peak_vram_usage_gb": 12.3,
  "chunks_per_second": 4.56
}
```

#### 3.5.2 Local Output

**æª”æ¡ˆçµæ§‹**:

```
data/
â”œâ”€â”€ indexes/
â”‚   â”œâ”€â”€ bm25_index.pkl      # BM25 ç´¢å¼• + Tokenizer
â”‚   â””â”€â”€ index_stats.json    # ç´¢å¼•çµ±è¨ˆè³‡è¨Š
â””â”€â”€ chromadb/
    â””â”€â”€ financial_10k_v1/   # ChromaDB æŒä¹…åŒ–ç›®éŒ„
        â”œâ”€â”€ chroma.sqlite3
        â””â”€â”€ *.parquet
```

**index_stats.json ç¯„ä¾‹**:

```json
{
  "bm25": {
    "total_chunks": 8420,
    "vocabulary_size": 45230,
    "avg_chunk_length": 98.7,
    "k1": 1.5,
    "b": 0.75
  },
  "chromadb": {
    "collection_name": "financial_10k_v1",
    "total_vectors": 8420,
    "embedding_dim": 1024,
    "index_type": "HNSW",
    "disk_usage_mb": 342.5
  }
}
```

---

## 4. èˆ‡ä¸‹ä¸€éšæ®µçš„éŠœæ¥

å®Œæˆé›™ç´¢å¼•æ§‹å»ºå¾Œ,ç³»çµ±å·²å…·å‚™æª¢ç´¢èƒ½åŠ›ã€‚**éšæ®µä¸‰:æ··åˆæª¢ç´¢é‚è¼¯**å°‡:

1. æ•´åˆ Dense èˆ‡ Sparse å…©å¥—ç´¢å¼•çš„æŸ¥è©¢çµæœ
2. ä½¿ç”¨ Reciprocal Rank Fusion (RRF) èåˆåˆ†æ•¸
3. å¯¦ä½œ Alpha åƒæ•¸èª¿å„ªæ©Ÿåˆ¶,å¹³è¡¡èªæ„èˆ‡é—œéµå­—æ¬Šé‡

> **é—œéµä¾è³´**:
>
> - éšæ®µä¸‰éœ€è¦åŒæ™‚è¼‰å…¥ ChromaDB Collection èˆ‡ BM25 Index
> - æŸ¥è©¢æ™‚å°‡ä¸¦è¡ŒåŸ·è¡Œå…©å¥—æª¢ç´¢,å†èåˆçµæœ
> - `metadata.has_table` ç­‰æ¬„ä½å°‡å½±éŸ¿åˆ†æ•¸åŠ æ¬Šç­–ç•¥
