# éšæ®µå…­:éƒ¨ç½²èˆ‡ä»‹é¢ (Deployment & Interface)

## 1. ç³»çµ±å®šä½èˆ‡æ ¸å¿ƒåƒ¹å€¼ (Context & Value)

### 1.1 éšæ®µå®šä½

éƒ¨ç½²èˆ‡ä»‹é¢æ˜¯ RAG ç³»çµ±çš„æœ€çµ‚äº¤ä»˜éšæ®µ,è² è²¬å°‡å‰äº”å€‹éšæ®µå»ºæ§‹çš„æ ¸å¿ƒèƒ½åŠ›å°è£ç‚ºå¯ç”¨çš„æœå‹™,ä¸¦é€éå‹å–„çš„ä½¿ç”¨è€…ä»‹é¢è®“çµ‚ç«¯ä½¿ç”¨è€…èƒ½å¤ è¼•é¬†å­˜å–è²¡å‹™å•ç­”åŠŸèƒ½ã€‚é€™å€‹éšæ®µæ±ºå®šäº†ç³»çµ±çš„å¯ç”¨æ€§èˆ‡ä½¿ç”¨è€…é«”é©—ã€‚

### 1.2 æ ¸å¿ƒç—›é»

å°‡ RAG ç³»çµ±å¾é–‹ç™¼ç’°å¢ƒé·ç§»è‡³ç”Ÿç”¢ç’°å¢ƒé¢è‡¨ä»¥ä¸‹æŒ‘æˆ°:

**æŒ‘æˆ°ä¸€:æœå‹™åŒ–**

- **å•é¡Œ**:å¦‚ä½•å°‡ Jupyter Notebook ä¸­çš„åŸå‹è½‰æ›ç‚ºç©©å®šçš„ API æœå‹™?
- **éœ€æ±‚**:é«˜å¯ç”¨æ€§ã€éŒ¯èª¤è™•ç†ã€è«‹æ±‚é©—è­‰

**æŒ‘æˆ°äºŒ:ä½¿ç”¨è€…é«”é©—**

- **å•é¡Œ**:å¦‚ä½•è®“éæŠ€è¡“ä½¿ç”¨è€…ä¹Ÿèƒ½è¼•é¬†ä½¿ç”¨ç³»çµ±?
- **éœ€æ±‚**:ç›´è§€çš„ä»‹é¢ã€å³æ™‚å›é¥‹ã€ä¾†æºè¿½æº¯

**æŒ‘æˆ°ä¸‰:æ•ˆèƒ½èˆ‡æˆæœ¬**

- **å•é¡Œ**:å¦‚ä½•åœ¨æœ‰é™è³‡æºä¸‹æ”¯æ’å¤šä½¿ç”¨è€…ä¸¦ç™¼?
- **éœ€æ±‚**:å¿«å–æ©Ÿåˆ¶ã€è² è¼‰å¹³è¡¡ã€æˆæœ¬æ§åˆ¶

**è§£æ±ºæ–¹æ¡ˆ**:

1. **FastAPI Backend**:æä¾› RESTful API,æ”¯æ´éåŒæ­¥è™•ç†
2. **Streamlit Frontend**:å¿«é€Ÿå»ºæ§‹äº’å‹•å¼ Web ä»‹é¢
3. **ç›£æ§èˆ‡æ—¥èªŒ**:å³æ™‚è¿½è¹¤ç³»çµ±å¥åº·ç‹€æ…‹èˆ‡ä½¿ç”¨æƒ…æ³

### 1.3 é æœŸæ•ˆæœ

å®Œæˆæœ¬éšæ®µå¾Œ,ç³»çµ±å°‡å…·å‚™:

1. **ç”Ÿç”¢ç´š API**:ç©©å®šçš„ `/search` èˆ‡ `/chat` Endpoints
2. **å‹å–„ä»‹é¢**:æ”¯æ´å°è©±æ­·å²ã€ä¾†æºè·³è½‰ã€Confidence é¡¯ç¤º
3. **å¯è§€æ¸¬æ€§**:å®Œæ•´çš„æ—¥èªŒã€æŒ‡æ¨™èˆ‡è¿½è¹¤
4. **å¯æ“´å±•æ€§**:æ”¯æ´æ°´å¹³æ“´å±•èˆ‡è² è¼‰å¹³è¡¡

---

> **æ¶æ§‹éŠœæ¥èªªæ˜**:
> äº†è§£éƒ¨ç½²çš„æŒ‘æˆ°å¾Œ,ä¸‹ä¸€å±¤å°‡èªªæ˜å¦‚ä½•é€éã€ŒBackend æœå‹™â†’Frontend ä»‹é¢â†’ç›£æ§ç³»çµ±ã€çš„ä¸‰å±¤æ¶æ§‹,å¯¦ç¾å®Œæ•´çš„ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²ã€‚

---

## 2. å·¥ä½œæµç¨‹èˆ‡æ¶æ§‹ (Workflow & Architecture)

### 2.1 æ•´é«”æ¶æ§‹

éƒ¨ç½²æ¶æ§‹æ¡ç”¨å‰å¾Œç«¯åˆ†é›¢è¨­è¨ˆ:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Streamlit Web Interface                  â”‚   â”‚
â”‚  â”‚  - Chat UI                                       â”‚   â”‚
â”‚  â”‚  - Source Citation Display                       â”‚   â”‚
â”‚  â”‚  - Confidence Score Visualization                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         FastAPI Application                      â”‚   â”‚
â”‚  â”‚  - /search: å–®æ¬¡æŸ¥è©¢                             â”‚   â”‚
â”‚  â”‚  - /chat: å°è©±å¼å•ç­”                             â”‚   â”‚
â”‚  â”‚  - /health: å¥åº·æª¢æŸ¥                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         RAG Pipeline                             â”‚   â”‚
â”‚  â”‚  Retrieval â†’ Reranking â†’ Generation              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ChromaDB   â”‚  â”‚ BM25 Index  â”‚  â”‚   Cache     â”‚     â”‚
â”‚  â”‚  (Vectors)  â”‚  â”‚  (Sparse)   â”‚  â”‚   (Redis)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Monitoring Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Logging   â”‚  â”‚   Metrics   â”‚  â”‚   Tracing   â”‚     â”‚
â”‚  â”‚  (File/DB)  â”‚  â”‚ (Prometheus)â”‚  â”‚  (Jaeger)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Backend: FastAPI æœå‹™

**ç‚ºä½•é¸æ“‡ FastAPI?**

| ç‰¹æ€§         | FastAPI       | Flask     | Django    |
| ------------ | ------------- | --------- | --------- |
| **æ•ˆèƒ½**     | é«˜ (éåŒæ­¥)   | ä¸­ (åŒæ­¥) | ä¸­ (åŒæ­¥) |
| **è‡ªå‹•æ–‡æª”** | âœ… (Swagger)  | âŒ        | âŒ        |
| **å‹åˆ¥æª¢æŸ¥** | âœ… (Pydantic) | âŒ        | éƒ¨åˆ†      |
| **å­¸ç¿’æ›²ç·š** | ä½            | ä½        | é«˜        |
| **é©ç”¨å ´æ™¯** | API æœå‹™      | å°å‹æ‡‰ç”¨  | å…¨ç«¯æ‡‰ç”¨  |

**API è¨­è¨ˆ**:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(
    title="Financial QA RAG API",
    description="10-K Financial Reports Question Answering System",
    version="1.0.0"
)

# è«‹æ±‚æ¨¡å‹
class SearchRequest(BaseModel):
    query: str
    top_k: int = 20
    alpha: float = 0.5
    include_sources: bool = True

class ChatRequest(BaseModel):
    query: str
    conversation_id: Optional[str] = None
    max_history: int = 5

# å›æ‡‰æ¨¡å‹
class Source(BaseModel):
    source_id: int
    chunk_id: str
    document: str
    section: str
    excerpt: str
    rerank_score: float

class SearchResponse(BaseModel):
    query: str
    answer: str
    sources: List[Source]
    confidence: dict
    metadata: dict

# Endpoints
@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    """
    å–®æ¬¡æŸ¥è©¢ Endpoint

    Args:
        request: æŸ¥è©¢è«‹æ±‚

    Returns:
        æŸ¥è©¢çµæœ
    """
    try:
        # åŸ·è¡Œ RAG Pipeline
        result = rag_pipeline.query(
            query=request.query,
            top_k=request.top_k,
            alpha=request.alpha
        )

        return SearchResponse(**result)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    å°è©±å¼å•ç­” Endpoint

    æ”¯æ´å¤šè¼ªå°è©±,è‡ªå‹•ç¶­è­·ä¸Šä¸‹æ–‡
    """
    try:
        # è¼‰å…¥å°è©±æ­·å²
        history = conversation_manager.get_history(request.conversation_id)

        # åŸ·è¡ŒæŸ¥è©¢
        result = rag_pipeline.query_with_history(
            query=request.query,
            history=history[-request.max_history:]
        )

        # å„²å­˜å°è©±
        conversation_manager.add_turn(
            conversation_id=request.conversation_id,
            query=request.query,
            answer=result['answer']
        )

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "chromadb": chromadb_client.heartbeat(),
        "bm25_loaded": bm25_index is not None
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2.3 Frontend: Streamlit ä»‹é¢

**Streamlit å„ªå‹¢**:

- ç´” Python é–‹ç™¼,ç„¡éœ€ HTML/CSS/JS
- å…§å»ºå…ƒä»¶è±å¯Œ (Chat, Sidebar, Metrics)
- è‡ªå‹•è™•ç†ç‹€æ…‹ç®¡ç†

**ä»‹é¢è¨­è¨ˆ**:

```python
import streamlit as st
import requests
from typing import List, Dict

# é é¢é…ç½®
st.set_page_config(
    page_title="Financial QA System",
    page_icon="ğŸ’°",
    layout="wide"
)

# å´é‚Šæ¬„é…ç½®
with st.sidebar:
    st.title("âš™ï¸ è¨­å®š")

    # æª¢ç´¢åƒæ•¸
    top_k = st.slider("Top-K", min_value=5, max_value=50, value=20)
    alpha = st.slider("Alpha (Dense æ¬Šé‡)", min_value=0.0, max_value=1.0, value=0.5, step=0.1)

    # æ¨¡å‹é¸æ“‡
    llm_model = st.selectbox("LLM æ¨¡å‹", ["gpt-4", "gpt-3.5-turbo", "claude-3"])

    # æ¸…é™¤æ­·å²
    if st.button("æ¸…é™¤å°è©±æ­·å²"):
        st.session_state.messages = []
        st.rerun()

# ä¸»æ¨™é¡Œ
st.title("ğŸ’° Financial QA System")
st.caption("åŸºæ–¼ 10-K è²¡å ±çš„æ™ºæ…§å•ç­”ç³»çµ±")

# åˆå§‹åŒ–å°è©±æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºå°è©±æ­·å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # é¡¯ç¤ºä¾†æºå¼•ç”¨
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("ğŸ“š æŸ¥çœ‹ä¾†æº"):
                for source in message["sources"]:
                    st.markdown(f"""
                    **[Source {source['source_id']}]** {source['document']} - {source['section']}

                    > {source['excerpt']}

                    *Rerank Score: {source['rerank_score']:.3f}*
                    """)

            # é¡¯ç¤º Confidence
            if "confidence" in message:
                conf = message["confidence"]
                st.metric(
                    "Confidence",
                    f"{conf['overall_confidence']:.2%}",
                    delta=conf['confidence_level']
                )

# ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # å‘¼å« API
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            response = requests.post(
                "http://localhost:8000/search",
                json={
                    "query": prompt,
                    "top_k": top_k,
                    "alpha": alpha
                }
            )

            if response.status_code == 200:
                result = response.json()

                # é¡¯ç¤ºç­”æ¡ˆ
                st.markdown(result["answer"])

                # å„²å­˜è‡³æ­·å²
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": result["answer"],
                    "sources": result["sources"],
                    "confidence": result["confidence"]
                })

                st.rerun()
            else:
                st.error(f"API éŒ¯èª¤: {response.status_code}")
```

### 2.4 å°è©±ç®¡ç†

**å°è©±æ­·å²å„²å­˜**:

```python
from typing import List, Dict, Optional
import json
from pathlib import Path

class ConversationManager:
    """å°è©±ç®¡ç†å™¨"""

    def __init__(self, storage_dir: Path):
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(exist_ok=True)

    def get_history(self, conversation_id: Optional[str]) -> List[Dict]:
        """å–å¾—å°è©±æ­·å²"""
        if not conversation_id:
            return []

        history_file = self.storage_dir / f"{conversation_id}.json"
        if not history_file.exists():
            return []

        with open(history_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def add_turn(
        self,
        conversation_id: str,
        query: str,
        answer: str
    ):
        """æ–°å¢å°è©±è¼ªæ¬¡"""
        history = self.get_history(conversation_id)

        history.append({
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "answer": answer
        })

        # å„²å­˜
        history_file = self.storage_dir / f"{conversation_id}.json"
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
```

---

> **ç´°ç¯€éŠœæ¥èªªæ˜**:
> ç¢ºç«‹äº†ã€ŒFastAPI Backend + Streamlit Frontendã€çš„æ¶æ§‹å¾Œ,ä»¥ä¸‹å°‡æ·±å…¥èªªæ˜éƒ¨ç½²é…ç½®ã€ç›£æ§ç³»çµ±èˆ‡æ•ˆèƒ½å„ªåŒ–ç­–ç•¥ã€‚

---

## 3. æŠ€è¡“è¦æ ¼èˆ‡å¯¦ä½œç´°ç¯€ (Detailed Specification)

### 3.1 éƒ¨ç½²é…ç½®

#### 3.1.1 Docker å®¹å™¨åŒ–

**Dockerfile (Backend)**:

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY . .

# ä¸‹è¼‰æ¨¡å‹ (å¯é¸,ä¹Ÿå¯æ›è¼‰ Volume)
RUN python -c "from transformers import AutoModel; AutoModel.from_pretrained('BAAI/bge-m3')"

# æš´éœ²ç«¯å£
EXPOSE 8000

# å•Ÿå‹•æœå‹™
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:

```yaml
version: "3.8"

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CHROMADB_HOST=chromadb
    depends_on:
      - chromadb
      - redis

  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    depends_on:
      - backend

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/chroma

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  chromadb_data:
  redis_data:
```

#### 3.1.2 ç’°å¢ƒè®Šæ•¸ç®¡ç†

**.env ç¯„ä¾‹**:

```bash
# LLM API
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Database
CHROMADB_HOST=localhost
CHROMADB_PORT=8001

# Cache
REDIS_HOST=localhost
REDIS_PORT=6379

# Application
LOG_LEVEL=INFO
MAX_WORKERS=4
CACHE_TTL=3600
```

**é…ç½®è¼‰å…¥**:

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    """æ‡‰ç”¨ç¨‹å¼è¨­å®š"""

    # LLM
    openai_api_key: str
    anthropic_api_key: str = None

    # Database
    chromadb_host: str = "localhost"
    chromadb_port: int = 8001

    # Cache
    redis_host: str = "localhost"
    redis_port: int = 6379
    cache_ttl: int = 3600

    # Application
    log_level: str = "INFO"
    max_workers: int = 4

    class Config:
        env_file = ".env"

settings = Settings()
```

### 3.2 å¿«å–ç­–ç•¥

#### 3.2.1 Redis å¿«å–

```python
import redis
import json
import hashlib

class QueryCache:
    """æŸ¥è©¢å¿«å–"""

    def __init__(self, redis_client: redis.Redis, ttl: int = 3600):
        self.redis = redis_client
        self.ttl = ttl

    def get_cache_key(self, query: str, params: dict) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        key_str = f"{query}_{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def get(self, query: str, params: dict) -> Optional[dict]:
        """å–å¾—å¿«å–"""
        key = self.get_cache_key(query, params)
        cached = self.redis.get(key)

        if cached:
            return json.loads(cached)
        return None

    def set(self, query: str, params: dict, result: dict):
        """è¨­å®šå¿«å–"""
        key = self.get_cache_key(query, params)
        self.redis.setex(
            key,
            self.ttl,
            json.dumps(result, ensure_ascii=False)
        )
```

#### 3.2.2 å¿«å–æ•´åˆè‡³ API

```python
@app.post("/search")
async def search_with_cache(request: SearchRequest):
    """å¸¶å¿«å–çš„æŸ¥è©¢"""

    # 1. æª¢æŸ¥å¿«å–
    cache_params = {
        "top_k": request.top_k,
        "alpha": request.alpha
    }

    cached_result = query_cache.get(request.query, cache_params)
    if cached_result:
        logger.info(f"Cache hit for query: {request.query}")
        return SearchResponse(**cached_result)

    # 2. åŸ·è¡ŒæŸ¥è©¢
    result = rag_pipeline.query(
        query=request.query,
        top_k=request.top_k,
        alpha=request.alpha
    )

    # 3. å„²å­˜å¿«å–
    query_cache.set(request.query, cache_params, result)

    return SearchResponse(**result)
```

### 3.3 ç›£æ§èˆ‡æ—¥èªŒ

#### 3.3.1 çµæ§‹åŒ–æ—¥èªŒ

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        # æª”æ¡ˆè™•ç†å™¨
        handler = logging.FileHandler('app.log')
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)

    def log_query(
        self,
        query: str,
        result: dict,
        latency_ms: float,
        cache_hit: bool
    ):
        """è¨˜éŒ„æŸ¥è©¢æ—¥èªŒ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": "query",
            "query": query,
            "latency_ms": latency_ms,
            "cache_hit": cache_hit,
            "confidence": result['confidence']['overall_confidence'],
            "num_sources": len(result['sources']),
            "tokens_used": result['metadata']['tokens_used']['total']
        }

        self.logger.info(json.dumps(log_entry, ensure_ascii=False))

    def log_error(self, error: Exception, context: dict):
        """è¨˜éŒ„éŒ¯èª¤æ—¥èªŒ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": "error",
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context
        }

        self.logger.error(json.dumps(log_entry, ensure_ascii=False))
```

#### 3.3.2 Prometheus æŒ‡æ¨™

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# å®šç¾©æŒ‡æ¨™
query_counter = Counter('rag_queries_total', 'Total number of queries')
query_latency = Histogram('rag_query_latency_seconds', 'Query latency in seconds')
cache_hit_counter = Counter('rag_cache_hits_total', 'Total cache hits')
confidence_gauge = Gauge('rag_confidence_score', 'Current confidence score')

@app.post("/search")
async def search_with_metrics(request: SearchRequest):
    """å¸¶æŒ‡æ¨™çš„æŸ¥è©¢"""

    start_time = time.time()

    try:
        # åŸ·è¡ŒæŸ¥è©¢
        result = rag_pipeline.query(request.query)

        # è¨˜éŒ„æŒ‡æ¨™
        query_counter.inc()
        query_latency.observe(time.time() - start_time)
        confidence_gauge.set(result['confidence']['overall_confidence'])

        return SearchResponse(**result)

    except Exception as e:
        logger.log_error(e, {"query": request.query})
        raise
```

#### 3.3.3 ç›£æ§å„€è¡¨æ¿

**Grafana Dashboard é…ç½®**:

```json
{
  "dashboard": {
    "title": "RAG System Monitoring",
    "panels": [
      {
        "title": "Query Rate",
        "targets": [
          {
            "expr": "rate(rag_queries_total[5m])"
          }
        ]
      },
      {
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rag_query_latency_seconds)"
          }
        ]
      },
      {
        "title": "Cache Hit Rate",
        "targets": [
          {
            "expr": "rate(rag_cache_hits_total[5m]) / rate(rag_queries_total[5m])"
          }
        ]
      },
      {
        "title": "Average Confidence",
        "targets": [
          {
            "expr": "avg_over_time(rag_confidence_score[5m])"
          }
        ]
      }
    ]
  }
}
```

### 3.4 æ•ˆèƒ½å„ªåŒ–

#### 3.4.1 éåŒæ­¥è™•ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncRAGPipeline:
    """éåŒæ­¥ RAG Pipeline"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def query_async(self, query: str) -> dict:
        """éåŒæ­¥æŸ¥è©¢"""

        # 1. ä¸¦è¡ŒåŸ·è¡Œæª¢ç´¢
        dense_task = asyncio.create_task(self._dense_search_async(query))
        sparse_task = asyncio.create_task(self._sparse_search_async(query))

        dense_results, sparse_results = await asyncio.gather(dense_task, sparse_task)

        # 2. èåˆçµæœ
        fused_results = self._fuse_results(dense_results, sparse_results)

        # 3. Reranking (åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡Œ,é¿å…é˜»å¡)
        loop = asyncio.get_event_loop()
        reranked = await loop.run_in_executor(
            self.executor,
            self._rerank,
            query,
            fused_results
        )

        # 4. LLM ç”Ÿæˆ
        answer = await self._generate_async(query, reranked)

        return answer

    async def _dense_search_async(self, query: str):
        """éåŒæ­¥å‘é‡æª¢ç´¢"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.dense_retriever.search,
            query
        )
```

#### 3.4.2 é€£ç·šæ± 

```python
from chromadb import HttpClient

class ChromaDBPool:
    """ChromaDB é€£ç·šæ± """

    def __init__(self, host: str, port: int, pool_size: int = 10):
        self.clients = [
            HttpClient(host=host, port=port)
            for _ in range(pool_size)
        ]
        self.current_idx = 0

    def get_client(self) -> HttpClient:
        """å–å¾—å®¢æˆ¶ç«¯ (Round-robin)"""
        client = self.clients[self.current_idx]
        self.current_idx = (self.current_idx + 1) % len(self.clients)
        return client
```

### 3.5 å®‰å…¨æ€§

#### 3.5.1 Rate Limiting

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/search")
@limiter.limit("10/minute")  # æ¯åˆ†é˜æœ€å¤š 10 æ¬¡è«‹æ±‚
async def search_with_rate_limit(request: Request, search_request: SearchRequest):
    """å¸¶é€Ÿç‡é™åˆ¶çš„æŸ¥è©¢"""
    return await search(search_request)
```

#### 3.5.2 è¼¸å…¥é©—è­‰

```python
from pydantic import BaseModel, validator

class SearchRequest(BaseModel):
    query: str
    top_k: int = 20
    alpha: float = 0.5

    @validator('query')
    def validate_query(cls, v):
        """é©—è­‰æŸ¥è©¢"""
        if not v or len(v.strip()) == 0:
            raise ValueError("Query cannot be empty")
        if len(v) > 500:
            raise ValueError("Query too long (max 500 characters)")
        return v.strip()

    @validator('top_k')
    def validate_top_k(cls, v):
        """é©—è­‰ Top-K"""
        if v < 1 or v > 100:
            raise ValueError("top_k must be between 1 and 100")
        return v

    @validator('alpha')
    def validate_alpha(cls, v):
        """é©—è­‰ Alpha"""
        if v < 0 or v > 1:
            raise ValueError("alpha must be between 0 and 1")
        return v
```

### 3.6 éƒ¨ç½²æª¢æŸ¥æ¸…å–®

#### 3.6.1 ä¸Šç·šå‰æª¢æŸ¥

```markdown
## éƒ¨ç½²æª¢æŸ¥æ¸…å–®

### åŠŸèƒ½æ¸¬è©¦

- [ ] API Endpoints æ­£å¸¸é‹ä½œ
- [ ] Frontend ä»‹é¢æ­£å¸¸é¡¯ç¤º
- [ ] ä¾†æºå¼•ç”¨æ­£ç¢ºé€£çµ
- [ ] Confidence Score è¨ˆç®—æ­£ç¢º

### æ•ˆèƒ½æ¸¬è©¦

- [ ] å–®æ¬¡æŸ¥è©¢å»¶é² < 3s
- [ ] ä¸¦ç™¼ 10 ä½¿ç”¨è€…ç„¡éŒ¯èª¤
- [ ] å¿«å–å‘½ä¸­ç‡ > 30%

### å®‰å…¨æ€§

- [ ] API Key å·²è¨­å®šç’°å¢ƒè®Šæ•¸
- [ ] Rate Limiting å·²å•Ÿç”¨
- [ ] è¼¸å…¥é©—è­‰å·²å¯¦ä½œ

### ç›£æ§

- [ ] æ—¥èªŒæ­£å¸¸å¯«å…¥
- [ ] Prometheus æŒ‡æ¨™å¯å­˜å–
- [ ] Grafana å„€è¡¨æ¿æ­£å¸¸é¡¯ç¤º

### æ–‡æª”

- [ ] API æ–‡æª”å·²ç”Ÿæˆ (Swagger)
- [ ] ä½¿ç”¨è€…æ‰‹å†Šå·²æ’°å¯«
- [ ] æ•…éšœæ’é™¤æŒ‡å—å·²æº–å‚™
```

---

## 4. å°ˆæ¡ˆç¸½çµ

å®Œæˆéšæ®µå…­å¾Œ,æ•´å€‹ Financial-QA-10k-RAG ç³»çµ±å·²å…·å‚™å®Œæ•´çš„ç”Ÿç”¢èƒ½åŠ›:

### 4.1 ç³»çµ±èƒ½åŠ›ç¸½è¦½

| éšæ®µ       | æ ¸å¿ƒèƒ½åŠ›     | é—œéµç”¢å‡º                       |
| ---------- | ------------ | ------------------------------ |
| **éšæ®µä¸€** | è³‡æ–™å·¥ç¨‹     | é«˜å“è³ª Chunks (8420 å€‹)        |
| **éšæ®µäºŒ** | é›™ç´¢å¼•æ§‹å»º   | BGE-M3 Embeddings + BM25 Index |
| **éšæ®µä¸‰** | æ··åˆæª¢ç´¢     | RRF/Weighted Sum èåˆæ¼”ç®—æ³•    |
| **éšæ®µå››** | é‡æ’åºèˆ‡ç”Ÿæˆ | Cross-Encoder + GPT-4 ç”Ÿæˆ     |
| **éšæ®µäº”** | ç³»çµ±è©•ä¼°     | RAGAS Score 0.78 (ç›®æ¨™ > 0.75) |
| **éšæ®µå…­** | éƒ¨ç½²èˆ‡ä»‹é¢   | FastAPI + Streamlit ç”Ÿç”¢æœå‹™   |

### 4.2 æŠ€è¡“äº®é»

1. **æ··åˆæª¢ç´¢æ¶æ§‹**:çµåˆ Dense èˆ‡ Sparse ç´¢å¼•,å…¼é¡§èªæ„ç†è§£èˆ‡ç²¾ç¢ºåŒ¹é…
2. **Kaggle-Local åˆ†æµ**:GPU å¯†é›†ä»»å‹™åœ¨ Kaggle åŸ·è¡Œ,æœ¬åœ°åƒ…éœ€ CPU
3. **å¯è§€æ¸¬æ€§**:å®Œæ•´çš„æ—¥èªŒã€æŒ‡æ¨™èˆ‡è¿½è¹¤ç³»çµ±
4. **ä½¿ç”¨è€…å‹å–„**:Streamlit ä»‹é¢æ”¯æ´ä¾†æºè¿½æº¯èˆ‡ Confidence é¡¯ç¤º

### 4.3 æœªä¾†å„ªåŒ–æ–¹å‘

1. **æ¨¡å‹å¾®èª¿**:é‡å°è²¡å ±é ˜åŸŸ Fine-tune BGE-M3 èˆ‡ Reranker
2. **å¤šæ¨¡æ…‹æ”¯æ´**:è™•ç†è²¡å ±ä¸­çš„åœ–è¡¨èˆ‡è¦–è¦ºåŒ–å…§å®¹
3. **å³æ™‚æ›´æ–°**:æ”¯æ´å¢é‡ç´¢å¼•æ›´æ–°,ç„¡éœ€é‡å»ºæ•´å€‹è³‡æ–™åº«
4. **å¤šèªè¨€æ”¯æ´**:æ“´å±•è‡³ä¸­æ–‡ã€æ—¥æ–‡ç­‰å…¶ä»–èªè¨€çš„è²¡å ±

---

**æ­å–œ!æ‚¨å·²å®Œæˆ Financial-QA-10k-RAG ç³»çµ±çš„å®Œæ•´æŠ€è¡“è¦åŠƒã€‚**
